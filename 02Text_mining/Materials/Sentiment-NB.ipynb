{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d28dfc",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Scikit-Learn Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6965bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "IMDB_DATA_FILE = DATA_DIR + 'imdb_labelled.txt'\n",
    "YELP_DATA_FILE = DATA_DIR + 'yelp_labelled.txt'\n",
    "AMAZON_DATA_FILE = DATA_DIR + 'amazon_cells_labelled.txt'\n",
    "\n",
    "COLUMN_NAMES = ['Review', 'Sentiment']\n",
    "yelp_reviews = pd.read_table(YELP_DATA_FILE, names=COLUMN_NAMES)\n",
    "amazon_reviews = pd.read_table(AMAZON_DATA_FILE, names=COLUMN_NAMES)\n",
    "imdb_reviews = pd.read_table(IMDB_DATA_FILE, names=COLUMN_NAMES)\n",
    "review_data = pd.concat([amazon_reviews, imdb_reviews, yelp_reviews], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tokens = review_data.Review.apply(func=word_tokenize).explode().str.lower()\n",
    "tokens = tokens[~tokens.isin(stopwords.words('english'))]\n",
    "review_data.Review = tokens.groupby(level=0).agg(lambda x: ' '.join(x))\n",
    "review_data.Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4dbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(review_data.Review, \n",
    "                                                    review_data.Sentiment, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e001c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower())\n",
    "    text = text.replace('hadn t' , 'had not')\\\n",
    "               .replace('wasn t', 'was not')\\\n",
    "               .replace('didn t', 'did not')\n",
    "    return text\n",
    "\n",
    "review_model_data = review_data.copy()\n",
    "review_model_data.Review = review_model_data.Review.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa43e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelining\n",
    "tfidf = TfidfVectorizer()\n",
    "clf = MultinomialNB()\n",
    "nb_tfidf = Pipeline([('vect', tfidf), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB applied\n",
    "nb_tfidf.fit(X_train.values, y_train.values)\n",
    "test_accuracy = nb_tfidf.score(X_test.values, y_test.values)\n",
    "'The model has a test accuracy of {:.0%}'.format(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773aeed",
   "metadata": {},
   "source": [
    "# Keras Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd067b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense\n",
    "# Define 10 restaurant reviews\n",
    "reviews =['Never coming back!', 'horrible service', 'rude waitress', 'cold food', \n",
    "          'horrible food!', 'awesome', 'awesome services!', 'rocks', 'poor work',\n",
    "          'couldn\\'t have done better' ]\n",
    "\n",
    "#Define labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "Vocab_size = 50\n",
    "encoded_reviews = [one_hot(d,Vocab_size) for d in reviews]\n",
    "print(f'encoded reviews: {encoded_reviews}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "padded_reviews = pad_sequences(encoded_reviews,maxlen=max_length,padding='post')\n",
    "print(padded_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=Vocab_size,output_dim=8,input_length=max_length)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271679e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_reviews,labels,epochs=100,verbose=0)\n",
    "print(embedding_layer.get_weights()[0].shape) # (50, 8)\n",
    "embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfd359-50ab-41f8-9e34-f2117103bba3",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data\n",
    "X = review_data[\"Review\"].astype(str).values\n",
    "y, classes = pd.factorize(review_data[\"Sentiment\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# vectorizer\n",
    "vec = tf.keras.layers.TextVectorization(max_tokens=10000, output_mode=\"int\", output_sequence_length=60)\n",
    "vec.adapt(X_train)\n",
    "vocab_size = len(vec.get_vocabulary())\n",
    "binary = (len(np.unique(y_train)) == 2)\n",
    "\n",
    "# model\n",
    "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "x = vec(inp)\n",
    "x = tf.keras.layers.Embedding(vocab_size, 32, mask_zero=True)(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dense(16, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "out = tf.keras.layers.Dense(1 if binary else len(classes), activation=\"sigmoid\" if binary else \"softmax\")(x)\n",
    "model = tf.keras.Model(inp, out)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\" if binary else \"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# train & evaluate\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.1, verbose=1)\n",
    "acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(f\"Test accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3939577-ce4c-4208-a64d-a3de1abe249e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
