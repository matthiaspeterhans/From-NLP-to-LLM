{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Intro: use your own datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/wjbmattingly/freecodecamp_spacy\n",
    "import numpy as np \n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1: Named Entity Recognition (NER) with spaCy\n",
    "- Which entities did spaCy detect?\n",
    "- Which important terms were missed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Example medical text\n",
    "text = \"The patient was prescribed 5 mg of Prednisone in Zurich.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print detected entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Rule-Based Matcher\n",
    "- Describe the what the following code is doing.\n",
    "- What are problems with coding like that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add EntityRuler properly (v3 syntax)\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Define patterns\n",
    "dosage_pattern = [{\"LIKE_NUM\": True}, {\"LOWER\": {\"IN\": [\"mg\", \"ml\", \"g\"]}}]\n",
    "med_suffix_pattern = [{\"TEXT\": {\"REGEX\": \"(?i).*(ine|ol|pril|sartan|mab)$\"}}]\n",
    "med_list_pattern = [{\"LOWER\": {\"IN\": [\"prednisone\", \"ibuprofen\", \"paracetamol\", \"metformin\"]}}]\n",
    "route_pattern = [{\"LOWER\": {\"IN\": [\"po\", \"iv\", \"im\", \"sc\"]}}]\n",
    "freq_pattern = [{\"LOWER\": {\"IN\": [\"once\", \"twice\"]}}, {\"LOWER\": {\"IN\": [\"daily\", \"weekly\"]}}]\n",
    "\n",
    "# Add patterns\n",
    "ruler.add_patterns([\n",
    "    {\"label\": \"DOSAGE\", \"pattern\": dosage_pattern},\n",
    "    {\"label\": \"MEDICATION\", \"pattern\": med_suffix_pattern},\n",
    "    {\"label\": \"MEDICATION\", \"pattern\": med_list_pattern},\n",
    "    {\"label\": \"ROUTE\", \"pattern\": route_pattern},\n",
    "    {\"label\": \"FREQUENCY\", \"pattern\": freq_pattern},\n",
    "])\n",
    "\n",
    "# Example text\n",
    "text = \"The patient was prescribed 5 mg of Prednisone in Zurich. Later, they took 10 ml ibuprofen PO twice daily.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"- {ent.text!r:>12}  ->  {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Train a tiny custom NER model in spaCy\n",
    "- Run the code and inspect output.\n",
    "- Try adding 5–6 new entity types (e.g., SYMPTOM, LAB_TEST, DEVICE, ROUTE, DURATION, FREQUENCY) with just 1–2 examples each to see how training reacts.\n",
    "- Evaluate qualitatively: What does the model get right/wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"The patient received 5 mg Prednisone.\", \n",
    "     {\"entities\": [(20, 24, \"DOSAGE\"), (25, 35, \"DRUG\")]}),\n",
    "    (\"Pain in the left knee improved after ibuprofen.\", \n",
    "     {\"entities\": [(12, 21, \"BODY_PART\"), (33, 42, \"DRUG\")]}),\n",
    "    (\"He was given 2 ml epinephrine IM.\", \n",
    "     {\"entities\": [(12, 16, \"DOSAGE\"), (17, 27, \"DRUG\")]}),\n",
    "    (\"CT showed a 2 cm lesion in the liver.\", \n",
    "     {\"entities\": [(12, 16, \"MEASUREMENT\"), (31, 36, \"BODY_PART\")]}),\n",
    "    (\"Administer 10 mg morphine intravenously.\", \n",
    "     {\"entities\": [(11, 16, \"DOSAGE\"), (17, 25, \"DRUG\")]}),\n",
    "    (\"MRI confirmed swelling in the brain.\", \n",
    "     {\"entities\": [(28, 33, \"BODY_PART\")]}),\n",
    "    (\"Patient reported headache, treated with aspirin.\", \n",
    "     {\"entities\": [(17, 25, \"BODY_PART\"), (40, 47, \"DRUG\")]}),\n",
    "    (\"She received 250 mg amoxicillin for 5 days.\", \n",
    "     {\"entities\": [(13, 19, \"DOSAGE\"), (20, 31, \"DRUG\")]}),\n",
    "    (\"X-ray revealed fracture in the right arm.\", \n",
    "     {\"entities\": [(35, 43, \"BODY_PART\")]}),\n",
    "    (\"The doctor prescribed 20 mg omeprazole daily.\", \n",
    "     {\"entities\": [(24, 29, \"DOSAGE\"), (30, 40, \"DRUG\")]}),\n",
    "    (\"Ultrasound detected a 5 cm cyst in the kidney.\", \n",
    "     {\"entities\": [(24, 28, \"MEASUREMENT\"), (40, 46, \"BODY_PART\")]}),\n",
    "    (\"Patient complained of chest pain, given nitroglycerin.\", \n",
    "     {\"entities\": [(22, 27, \"BODY_PART\"), (35, 48, \"DRUG\")]}),\n",
    "    (\"Treatment started with 8 mg dexamethasone.\", \n",
    "     {\"entities\": [(22, 26, \"DOSAGE\"), (27, 40, \"DRUG\")]}),\n",
    "    (\"Examination revealed tumor in the stomach.\", \n",
    "     {\"entities\": [(32, 39, \"BODY_PART\")]}),\n",
    "    (\"She was prescribed 50 mg sertraline at night.\", \n",
    "     {\"entities\": [(19, 24, \"DOSAGE\"), (25, 35, \"DRUG\")]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "# from train_data import TRAIN_DATA\n",
    "\n",
    "# Blank English pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Convert to spaCy examples\n",
    "examples = []\n",
    "for text, ann in TRAIN_DATA:\n",
    "    doc = nlp.make_doc(text)\n",
    "    examples.append(Example.from_dict(doc, ann))\n",
    "\n",
    "# Training loop\n",
    "optimizer = nlp.initialize()\n",
    "for epoch in range(20):\n",
    "    random.shuffle(examples)\n",
    "    losses = {}\n",
    "    for ex in examples:\n",
    "        nlp.update([ex], sgd=optimizer, losses=losses)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Losses: {losses}\")\n",
    "\n",
    "# Test on new text\n",
    "test_text = \"The nurse gave 10 mg morphine for arm pain.\"\n",
    "doc = nlp(test_text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extras with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://spacy.io/api/annotation#pos-tagging\n",
    "sentence1 = list(doc.sents)[0]\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import spacy\n",
    "\n",
    "def fetch_wikipedia_extract(title: str, lang: str = \"de\") -> str:\n",
    "    url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1,         \n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": 2\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"YourAppName/1.0 (contact@example.com)\"\n",
    "    }\n",
    "    r = requests.get(url, params=params, headers=headers, timeout=15)\n",
    "    r.raise_for_status()  # wirft HTTPError bei 4xx/5xx\n",
    "\n",
    "    # Should be JSON \n",
    "    ctype = r.headers.get(\"Content-Type\", \"\")\n",
    "    if \"application/json\" not in ctype:\n",
    "        # Debuggen\n",
    "        snippet = r.text[:300].replace(\"\\n\", \" \")\n",
    "        raise ValueError(f\"Unexpected content-type: {ctype}. First bytes: {snippet!r}\")\n",
    "\n",
    "    data = r.json()\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", [])\n",
    "    if not pages or \"extract\" not in pages[0]:\n",
    "        raise ValueError(f\"No extract found for title={title!r}\")\n",
    "    return pages[0][\"extract\"]\n",
    "\n",
    "text = fetch_wikipedia_extract(\"Sprachtechnologie\", lang=\"de\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(text)\n",
    "print(\"Tokens:\", len(doc))\n",
    "print(\"First 200 characters:\", text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace path\n",
    "import spacy\n",
    "with open (\"/Users/sym3/weine-xml-2024.xml\", \"r\") as f:\n",
    "    text = f.read()\n",
    "doc = nlp(text)\n",
    "sentence1 = list(doc.sents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "your_word = \"cat\"\n",
    "ms = nlp.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10)\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spacy Pipeline\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NER with rules\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_EMAIL\": True}]\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
    "doc = nlp(\"This is an email address: wmattingly@aol.com\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (nlp.vocab[matches[0][0]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[matches[0][1]:matches[0][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RegEx\n",
    "import re\n",
    "pattern = r\"(((\\d){1,2}( (January|February|March|April|May|June|July|August|September|October|November|December)))|(((January|February|March|April|May|June|July|August|September|October|November|December) )(\\d){1,2}))\"\n",
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "print (iter_matches)\n",
    "for hit in iter_matches:\n",
    "    print (hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "for hit in iter_matches:\n",
    "    start = hit.start()\n",
    "    end = hit.end()\n",
    "    print (text[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "print(doc.ents)\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(text)\n",
    "original_ents = list(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwt_ents = []\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start, span.end, span.text))\n",
    "     \n",
    "### Inject the Spans into the doc.ents\n",
    "for ent in mwt_ents:\n",
    "    start, end, name = ent\n",
    "    per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "    original_ents.append(per_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = original_ents\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom component\n",
    "from spacy.language import Language\n",
    "@Language.component(\"paul_ner\")\n",
    "def paul_ner(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "    pattern = r\"Paul [A-Z]\\w+\"\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))  \n",
    "    ### Inject the Spans into the doc.ents\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.blank(\"en\")\n",
    "nlp2.add_pipe(\"paul_ner\")\n",
    "doc2 = nlp2(text)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom component with filter for overlaps\n",
    "from spacy.language import Language\n",
    "from spacy.util import filter_spans\n",
    "@Language.component(\"holly_ner\")\n",
    "def holly_ner(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "    pattern = r\"Hollywood\"\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))  \n",
    "    ### Inject the Spans into the doc.ents\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"CINEMA\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp3 = spacy.load(\"en_core_web_sm\")\n",
    "nlp3.add_pipe(\"holly_ner\")\n",
    "doc3 = nlp3(text)\n",
    "print(doc3.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc3.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
